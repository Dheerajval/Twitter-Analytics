# -*- coding: utf-8 -*-
"""Twitter Sentiment Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KLL_qQDWUSjoMdcTyKNyrnf4iHqNqNLC
"""

# STEP 1: Install required packages
!pip install pandas numpy scikit-learn nltk matplotlib seaborn --quiet

# STEP 2: Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
import csv
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import warnings # Added to suppress potential warnings from sklearn

warnings.filterwarnings('ignore') # Optional: Suppress warnings for cleaner output

# STEP 3: NLTK resources (Download only if not already downloaded)
try:
    stopwords.words('english')
except LookupError:
    print("Downloading NLTK stopwords...")
    nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# --- CORRECTED CSV LOADING ---
# STEP 4: Load CSV with default quoting to handle commas/quotes in text
file_path = '/content/training.1600000.processed.noemoticon.csv' # Make sure this path is correct for your environment
columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']

try:
    # CRITICAL FIX: Removed `quoting=csv.QUOTE_NONE` and `on_bad_lines='skip'`.
    # Using pandas defaults handles standard CSV quoting correctly.
    df = pd.read_csv(file_path, encoding='latin-1', names=columns)
    print(f"✅ Loaded shape: {df.shape}")

    # Basic check after loading
    if df['text'].isnull().any():
         print(f"⚠️ Warning: Found {df['text'].isnull().sum()} NaN values in 'text' column immediately after loading.")
    if df['sentiment'].isnull().any():
         print(f"⚠️ Warning: Found {df['sentiment'].isnull().sum()} NaN values in 'sentiment' column immediately after loading.")

except FileNotFoundError:
    print(f"❌ Error: The file was not found at {file_path}")
    print("Please ensure the file path is correct and the CSV file exists.")
    raise
except Exception as e:
    print(f"❌ Error loading CSV file: {e}")
    print("Check file path, encoding, and potential parsing issues (e.g., file corruption).")
    raise # Re-raise the exception to stop execution
# --- END OF CORRECTION ---

# STEP 5: Inspect the unique sentiment values directly (should be [0 4])
print("\n🔍 Unique sentiment values as loaded:")
print(df['sentiment'].unique())

# STEP 6: Inspect the first 10 rows to check the raw sentiment values and text
print("\n🛑 Showing first 10 rows for manual inspection:")
print(df.head(10))

# Check for NaN values in the 'text' column before explicit dropping
print("\n🔍 Number of NaN values in 'text' column before explicit dropping:")
initial_text_nan_count = df['text'].isnull().sum()
print(initial_text_nan_count)

# STEP 7: Fix sentiment column (Convert 0, 4 -> 0, 1)
print("\n🛠️ Processing sentiment values...")
original_sentiments = df['sentiment'].unique()
expected_sentiments = {0, 4}

if set(original_sentiments) == expected_sentiments:
    print(f"✅ Found expected sentiment values: {original_sentiments}. Mapping 0->0, 4->1.")
    # Map 4 to 1, leave 0 as 0.
    df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
elif set(original_sentiments).issubset({0, 1}):
    print("✅ Sentiment values already in binary format (0 and 1).")
else:
    # If loading failed, sentiments might be mixed types or incorrect
    print(f"⚠️ Warning: Unexpected sentiment values found: {original_sentiments}. Attempting conversion if possible, otherwise rows might be dropped.")
    # Attempt to convert numeric ones if they exist
    if 0 in original_sentiments and 4 in original_sentiments:
         df = df[df['sentiment'].isin([0, 4])] # Keep only rows with 0 or 4
         df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
         print("ℹ️ Kept rows with sentiment 0 or 4 and converted.")
    elif 0 in original_sentiments and 1 in original_sentiments:
         df = df[df['sentiment'].isin([0, 1])] # Keep only rows with 0 or 1
         print("ℹ️ Kept rows with sentiment 0 or 1.")
    else:
         # If still not matching, raise error as something fundamental is wrong
         raise ValueError(f"❌ Unhandled sentiment format. Unique values found: {original_sentiments}. Check CSV loading and file integrity.")

# After attempting the conversion, verify the unique sentiment values again
print("🔍 Unique sentiment values after processing:")
print(df['sentiment'].unique())

# Filter out any rows where sentiment is NaN (might happen if original data had non-numeric values)
initial_rows = len(df)
df.dropna(subset=['sentiment'], inplace=True)
rows_after_sentiment_dropna = len(df)
if initial_rows != rows_after_sentiment_dropna:
    print(f"⚠️ Dropped {initial_rows - rows_after_sentiment_dropna} rows due to NaN sentiment.")

# Ensure sentiment column is integer type
if not pd.api.types.is_integer_dtype(df['sentiment']):
    try:
        df['sentiment'] = df['sentiment'].astype(int)
        print("✅ Sentiment column successfully converted to integer type.")
    except ValueError:
        print("❌ Error: Could not convert sentiment column to integer after processing. Inspect data.")
        raise

print(f"📊 Dataset shape after ensuring valid sentiment: {df.shape}")

# STEP 8: Drop rows with NaN in 'text' column
initial_rows = len(df)
df.dropna(subset=['text'], inplace=True)
rows_after_text_dropna = len(df)
nan_dropped = initial_rows - rows_after_text_dropna
if nan_dropped > 0:
    print(f"✅ Dropped {nan_dropped} rows with NaN in 'text' column.")
elif initial_text_nan_count > 0:
     print(f"⚠️ Warning: Initial NaN count was {initial_text_nan_count}, but 0 rows dropped here. Check logic.")
else:
    print("✅ No rows with NaN 'text' found or dropped at this step.")

print(f"📊 Dataset shape after dropping NaN 'text': {df.shape}")

# STEP 9: Show sample BEFORE cleaning
print("\n📄 Sample tweets BEFORE cleaning:")
# Use min to avoid error if df has less than 5 rows after cleaning
sample_size = min(5, len(df))
if sample_size > 0:
    print(df['text'].sample(sample_size, random_state=42).to_list())
else:
    print("ℹ️ No data available to sample.")


# STEP 10: Clean tweets (using the conservative function from the prompt)
# You might want to adjust this regex later depending on desired cleaning level
def clean_tweet_conservative(tweet):
    tweet = str(tweet).lower() # Ensure string type and lowercase
    tweet = re.sub(r"http\S+|www\S+", '', tweet) # Remove URLs/links
    # Keep only lowercase letters, spaces, @, #. Remove others (numbers, punctuation etc.)
    tweet = re.sub(r"[^a-z\s@#]", '', tweet)
    # Remove stopwords (optional, can sometimes hurt performance depending on model/task)
    # tweet = ' '.join(word for word in tweet.split() if word not in stop_words)
    return tweet.strip() # Remove leading/trailing whitespace

print("\n🧹 Applying conservative text cleaning...")
df['clean_text'] = df['text'].apply(clean_tweet_conservative)

# Check for empty strings AFTER cleaning
non_empty_mask = df['clean_text'].str.strip().astype(bool)
empty_count = len(df) - non_empty_mask.sum()

if empty_count > 0:
    print(f"⚠️ Found {empty_count} tweets that became empty after cleaning. Removing them.")
    df = df[non_empty_mask] # Keep only rows where clean_text is not empty
else:
    print("✅ No tweets became empty after cleaning.")

# STEP 11: Confirm cleaned data shape
print(f"\n📊 Dataset shape after cleaning and removing empty tweets: {df.shape}")
if df.empty:
    # This error should not occur if STEP 4 is fixed and the dataset is valid
    raise ValueError("🚫 Dataset is empty after cleaning. Check cleaning function and original data integrity.")

# Show sample AFTER cleaning
print("\n📄 Sample tweets AFTER cleaning:")
sample_size = min(5, len(df))
if sample_size > 0:
    print(df['clean_text'].sample(sample_size, random_state=42).to_list())
else:
    print("ℹ️ No data available to sample.")


# STEP 12: Downsample (if dataset is large)
target_rows = 100000
if len(df) > target_rows:
    print(f"\n⚙️ Downsampling from {len(df)} to {target_rows} rows...")
    df = df.sample(n=target_rows, random_state=42)
    print(f"✅ Downsampled dataset shape: {df.shape}")
else:
    print(f"\n⚠️ Using full dataset ({len(df)} rows) as it's not larger than {target_rows}.")


# STEP 13: TF-IDF Vectorization
print("\n🤖 Performing TF-IDF Vectorization...")
# Limit features to prevent excessive memory usage/computation time
vectorizer = TfidfVectorizer(max_features=10000, stop_words=list(stop_words)) # You can experiment with/without stop_words here
X = vectorizer.fit_transform(df['clean_text'])
y = df['sentiment']
print(f"✅ TF-IDF matrix shape: {X.shape}")


# STEP 14: Train/Test Split
print("\n splitting data into Train/Test sets (80/20)...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Added stratify
print(f"📊 Train set shape: {X_train.shape}, Test set shape: {X_test.shape}")
print(f"📊 Train labels distribution: \n{y_train.value_counts(normalize=True)}")
print(f"📊 Test labels distribution: \n{y_test.value_counts(normalize=True)}")


# STEP 15: Train Logistic Regression Model
print("\n🧠 Training Logistic Regression model...")
# Increased max_iter for convergence, added class_weight for balance
model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', C=1.0) # C is regularization strength
model.fit(X_train, y_train)
print("✅ Model training complete.")


# STEP 16: Evaluate Model
print("\n⚖️ Evaluating model performance...")
y_pred = model.predict(X_test)
print("\n📊 Classification Report:")
# Added zero_division=0 to handle cases where a class might have no predicted samples (unlikely here)
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive"], zero_division=0))


# STEP 17: Confusion Matrix Visualization
print("\n📈 Plotting Confusion Matrix...")
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4)) # Adjusted figure size
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"],
            annot_kws={"size": 12}) # Made annotations larger
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('Actual Label', fontsize=12)
plt.title('Confusion Matrix', fontsize=14)
plt.show()

print("\n🎉 Script execution finished.")